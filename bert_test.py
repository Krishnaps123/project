# -*- coding: utf-8 -*-
"""BERT_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11rs7CS-cmF_VeQqsp7b3_89ty_egJL_D
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install tweet-preprocessor

!pip install pytorch-transformers
!pip install transformers

import tensorflow as tf

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import preprocessor as p

from transformers import XLNetTokenizer, XLNetForSequenceClassification, DebertaTokenizer, DebertaForSequenceClassification, RobertaTokenizerFast, RobertaForSequenceClassification, ElectraTokenizer, ElectraForSequenceClassification
from transformers import AdamW

from tqdm import tqdm, trange
import pandas as pd
import io
import numpy as np
import matplotlib.pyplot as plt
# % matplotlib inline

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
n_gpu = torch.cuda.device_count()
torch.cuda.get_device_name(0)

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

df = pd.read_csv("/content/drive/MyDrive/projects/data/Constraint_Train.csv")
val_df = pd.read_csv("/content/drive/MyDrive/projects/data/Constraint_Val.csv")
test_df = pd.read_csv("/content/drive/MyDrive/projects/data/english_test_with_labels.csv")
test_label_df = pd.read_csv('/content/drive/MyDrive/projects/data/english_test_with_labels.csv')

df.shape, val_df.shape, test_df.shape

def preprocess(row):
  text = row['tweet']
  # text = text.strip('\xa0')
  text = p.clean(text)
  # text = re.sub(r'\([0-9]+\)', '', text).strip()
  return text

df.head()

df.iloc[1, 1]

test_df.head(2)

def map_label(row):
  return 0 if row['label']=='real' else 1

df['label_encoded'] = df.apply(lambda x: map_label(x), 1)
val_df['label_encoded'] = val_df.apply(lambda x: map_label(x), 1)
# test_df['label_encoded'] = test_df.apply(lambda x: map_label(x), 1)

train_sentences = df.tweet.values
train_token_ids = df.id.values
val_sentences = val_df.tweet.values
val_token_ids = val_df.id.values
test_sentences = test_df.tweet.values
test_token_ids = test_df.id.values



train_sentences = [sentence + " [SEP] [CLS]" for sentence in train_sentences]
train_labels = df.label_encoded.values
val_sentences = [sentence + " [SEP] [CLS]" for sentence in val_sentences]
val_labels = val_df.label_encoded.values
test_sentences = [sentence + " [SEP] [CLS]" for sentence in test_sentences]

def get_dataloader(network, train_sentences, val_sentences, train_labels, val_labels, test_sentences, train_token_ids, val_token_ids, test_token_ids):
  #dic = {"BERT": "bert-base-uncased"}
  if network=='BERT':
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

  tokenized_train_texts = [tokenizer.tokenize(sent) for sent in train_sentences]
  print ("Tokenize the first sentence:")
  print (tokenized_train_texts[0])

  tokenized_val_texts = [tokenizer.tokenize(sent) for sent in val_sentences]
  tokenized_test_texts = [tokenizer.tokenize(sent) for sent in test_sentences]
  input_train_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_train_texts]
  input_val_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_val_texts]
  input_test_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_test_texts]

  input_train_ids = pad_sequences(input_train_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
  input_val_ids = pad_sequences(input_val_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
  input_test_ids = pad_sequences(input_test_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")

  # Create attention masks
  train_attention_masks = []

  # Create a mask of 1s for each token followed by 0s for padding
  for seq in input_train_ids:
    seq_mask = [float(i>0) for i in seq]
    train_attention_masks.append(seq_mask)
  train_masks = np.array(train_attention_masks)

  # Create attention masks
  val_attention_masks = []

  # Create a mask of 1s for each token followed by 0s for padding
  for seq in input_val_ids:
    seq_mask = [float(i>0) for i in seq]
    val_attention_masks.append(seq_mask)
  validation_masks = np.array(val_attention_masks)

  # Create attention masks
  test_attention_masks = []

  # Create a mask of 1s for each token followed by 0s for padding
  for seq in input_test_ids:
    seq_mask = [float(i>0) for i in seq]
    test_attention_masks.append(seq_mask)
  test_masks = np.array(test_attention_masks)

  train_inputs = torch.tensor(input_train_ids)
  validation_inputs = torch.tensor(input_val_ids)
  test_inputs = torch.tensor(input_test_ids)
  train_labels = torch.tensor(train_labels)
  validation_labels = torch.tensor(val_labels)
  train_masks = torch.tensor(train_masks)
  validation_masks = torch.tensor(validation_masks)
  test_masks = torch.tensor(test_masks)

  train_data = TensorDataset(torch.tensor(train_token_ids), train_inputs, train_masks, train_labels)
  train_sampler = RandomSampler(train_data)
  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

  validation_data = TensorDataset(torch.tensor(val_token_ids), validation_inputs, validation_masks, validation_labels)
  validation_sampler = SequentialSampler(validation_data)
  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)

  test_data = TensorDataset(torch.tensor(test_token_ids), test_inputs, test_masks)
  test_sampler = SequentialSampler(test_data)
  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)

  return train_dataloader, validation_dataloader, test_dataloader

from transformers import XLMModel, BertTokenizer, BertForSequenceClassification, RobertaTokenizerFast, RobertaForSequenceClassification

MAX_LEN = 128
batch_size = 32
rain_dataloader_Electra, val_dataloader_Electra, test_dataloader_Electra = get_dataloader("BERT", train_sentences, val_sentences, train_labels, val_labels, test_sentences, train_token_ids, val_token_ids, test_token_ids)

model5 = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
directory_path = '/content/drive/MyDrive/projects'
model5.load_state_dict(torch.load(directory_path+'/projectsBERT_base_uncased_best_model.ckpt'))
model5.eval()
model5.cuda()

def get_model_preds_labels(model, dataloader, mode='train'):
  data_vectors = []
  labels = []
  ids = []

  with torch.no_grad():
      correct = 0
      total = 0
      for i, batch in enumerate(dataloader):
        batch = tuple(t.to(device) for t in batch)
        if mode=='test':
            token_ids, b_input_ids, b_input_mask = batch
            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)
            for point, tk_id in zip(F.softmax(outputs.logits).tolist(), token_ids.tolist()):
              data_vectors.append(point)
              ids.append(tk_id)
        else:
          # Unpack the inputs from our dataloader
          token_ids, b_input_ids, b_input_mask, b_labels = batch
          # Forward pass
          outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)
          for point, lab, tk_id in zip(F.softmax(outputs.logits).tolist(), b_labels.tolist(), token_ids.tolist()):
            data_vectors.append(point)
            labels.append(lab)
            ids.append(tk_id)

  if mode=='test':
    return data_vectors, ids
  else:
    return data_vectors, labels, ids

m5, _, train_ids5 = get_model_preds_labels(model5, rain_dataloader_Electra)

print(m5)

pred_labels = ["real" if np.argmax(np.array([each]), 1)[0]==0 else "fake" for each in m5]
train_pred_df = pd.DataFrame({'id': train_ids5, 'predicted_label': pred_labels})
train_pred_df = df[["id", "tweet","label"]].merge(train_pred_df, on='id', how='left')
train_pred_df.head()

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score

cm=confusion_matrix(train_pred_df['label'].values, train_pred_df['predicted_label'].values)

import seaborn as sns
f = sns.heatmap(cm, annot=True,fmt='d',cmap='Blues',xticklabels=['real','fake'], yticklabels=['real','fake'])

accuracy_score(train_pred_df['label'].values, train_pred_df['predicted_label'].values)

from sklearn.metrics import classification_report
print(classification_report(train_pred_df['label'].values, train_pred_df['predicted_label'].values, target_names=['real','fake']))

t5, t_ids5 = get_model_preds_labels(model5, test_dataloader_Electra, 'test')

test_pred_labels = ["real" if np.argmax(np.array([each]), 1)[0]==0 else "fake" for each in t5]
test_pred_df = pd.DataFrame({'id': t_ids5, 'predicted_label': test_pred_labels})
test_pred_df = test_label_df[["id", "label"]].merge(test_pred_df, on='id', how='left')
test_pred_df.head()

cm=confusion_matrix(test_pred_df['label'].values, test_pred_df['predicted_label'].values)
import seaborn as sns
f = sns.heatmap(cm, annot=True,fmt='d',cmap='Blues',xticklabels=['real','fake'], yticklabels=['real','fake'])

from sklearn.metrics import classification_report
print(classification_report(test_pred_df['label'].values, test_pred_df['predicted_label'].values, target_names=['real','fake']))

